model_name: "meta-llama/Meta-Llama-3.1-8B-Instruct"  # 'llama3.1_70b', "llama3.1_8b", "qwen2.5_72b", "qwen2.5_7b"
use_transformer_lens: false
padding_side: "left"
process_hidden_method: "last_assistant_to_eos_mean"  # "last_assistant_to_eos_each", "last"
clf: "pcascore"  # "pca", "lr"
pc_number: 3
normalize: true
batch_size: 16
all_pc_exp: [1, 2, 4, 8, 32, 128, 512]
n_train_examples: [0, 2, 4, 8, 16, 32, 64, 128, 256]
